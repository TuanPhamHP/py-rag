
from app.db.database import collection
from app.db.chromadb_store.chromadb_client import generate_embedding  # Import h√†m ƒë√£ c√≥
from typing import List
from pydantic import BaseModel
from rank_bm25 import BM25Okapi
import os

class RelevantDoc(BaseModel):
    filename: str
    file_path: str
    content: str


def split_into_chunks(document: str, chunk_size: int = 250) -> list:
    """Chia n·ªôi dung t√†i li·ªáu th√†nh ƒëo·∫°n nh·ªè (chunks)"""
    words = document.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

def rank_passages(passages: List[str], query: str, top_k: int = 3) -> List[str]:
    """X·∫øp h·∫°ng c√°c ƒëo·∫°n vƒÉn b·∫£n theo ƒë·ªô li√™n quan v·ªõi query"""
    tokenized_passages = [p.split() for p in passages]  # Tokenize t·ª´ng ƒëo·∫°n
    bm25 = BM25Okapi(tokenized_passages)  # T·∫°o index BM25
    query_tokens = query.split()  # Tokenize c√¢u h·ªèi
    scores = bm25.get_scores(query_tokens)  # T√≠nh ƒëi·ªÉm BM25

    # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë v√† l·∫•y top_k ƒëo·∫°n c√≥ ƒëi·ªÉm cao nh·∫•t
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    return [passages[i] for i in top_indices]

async def retrieve_context(query: str, top_k: int = 3) -> List[RelevantDoc]:
    """Truy v·∫•n ChromaDB b·∫±ng embedding ƒë·ªÉ l·∫•y top_k t√†i li·ªáu li√™n quan"""
    try:
        query_embedding = generate_embedding(query)  # Sinh embedding cho c√¢u h·ªèi
        results = collection.query(
            query_embeddings=[query_embedding], 
            n_results=top_k, 
            include=["documents", "metadatas"]
        )

        documents_list = results.get("documents", [[]])[0]  # L·∫•y danh s√°ch t√†i li·ªáu
        metadatas_list = results.get("metadatas", [[]])[0]  # L·∫•y metadata

        if not documents_list:
            return []
        
        # Chia nh·ªè t·ª´ng t√†i li·ªáu th√†nh ƒëo·∫°n nh·ªè (chunks)
        all_chunks = []
        for doc, meta in zip(documents_list, metadatas_list):
            chunks = split_into_chunks(doc)  # C·∫Øt nh·ªè t√†i li·ªáu
            for chunk in chunks:
                all_chunks.append({
                    "chunk": chunk,
                    "filename": meta.get("filename", "Unnamed"),
                    "file_path": meta.get("file_path", "N/A")
                })

        # üî• D√πng BM25 - sliding windows
        tokenized_chunks = [chunk["chunk"].split() for chunk in all_chunks]
        bm25 = BM25Okapi(tokenized_chunks)
        tokenized_query = query.split()
        scores = bm25.get_scores(tokenized_query)

        # L·∫•y top_k ƒëo·∫°n ph√π h·ª£p nh·∫•t
        best_chunks = sorted(zip(all_chunks, scores), key=lambda x: x[1], reverse=True)[:top_k]

        # Chu·∫©n h√≥a k·∫øt qu·∫£ th√†nh danh s√°ch `RelevantDoc`
        relevant_docs = [
            RelevantDoc(
                filename=chunk["filename"],
                file_path=chunk["file_path"],
                content=chunk["chunk"]
            )
            for chunk, _ in best_chunks
        ]

        return relevant_docs
    except Exception as e:
        print(f"Error in retrieve_context: {e}")
        return []

def search_documents(query: str, top_k: int = 15):
    """T√¨m ki·∫øm vƒÉn b·∫£n g·∫ßn nh·∫•t trong ChromaDB"""
    query_embedding = generate_embedding(query)  # D√πng h√†m c√≥ s·∫µn

    results = collection.query(
        query_embeddings=[query_embedding], 
        n_results=top_k
    )

    return results
